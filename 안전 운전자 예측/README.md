# ML Project - 안전 운전자 예측
> [프로젝트 kaggle 링크](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/submissions)

## 탐색적 데이터 분석

### 데이터 둘러보기

**학습 및 테스트 데이터**  
<p align="center">
  <img src=./images/train_data.png>
</p>

- 피처명 형식 : 피처명에 다양한 메타 정보가 포함되어 있다.
    - ps_ (분류) _ (분류별 일련번호) _ (데이터 종류)
    - 데이터 종류
        - bin : 이진 피처
        - cat : 명목형 피처
        - 생략 : 순서형 피처 또는 연속형 피처
    - 분류와 분류별 일련변호로는 정보를 확인하기 어렵다. 데이터 종류에서는 유의미한 정보를 얻을 수 있다.

**To-do**
- 결측값이 없다고 나와있지만, -1로 입력되어 있어 출력이 되지 않을 수 있다. -1을 np.NaN으로 변환한 후 개수를 세어본다.
    - missingno 패키지 사용
        - 결측값을 시각화하는 패키지
        - missingno.bar() : 결측값을 막대 그래프 형식으로 시각화
    - 훈련 데이터 값을 직접 바꾸지 않고 copy()를 사용해서 복사본으로 진행한다.

<p align="center">
  <img src=./images/missing_train.png>
</p>

- 막대 그래프 높이가 낮을수록 결측값이 많다는 뜻이다.
    - ps_reg_03, ps_car_03_cat, ps_car_05_cat 피처에 결측값이 많다.
- ps_car_14에 결측값이 조금 있고 나머지는 거의 존재하지 않는다.

**피처 요약표**  
<p align="center">
  <img src=./images/feature_summary.png>
</p>


### 데이터 시각화
**To-do**
- 타깃값 분포를 활용해 타깃값이 얼마나 불균형한지 알아본다.
    - 이진 피처, 명목형 피처, 순서형 피처의 고윳값별 타깃값 비율을 살펴본다.
- 고윳값별 타깃값 비율을 확인해 모델링 시 제거해야 할 피처를 선별한다.

**타깃값 분포**
<p align="center">
  <img src=./images/target_distribution.png>
</p>

- 전체 운전자 중 3.6%(target=1)만 보험금을 청구했다. 즉, 타깃값이 불균형하다.
    - 타깃값 1을 잘 예측하는 것이 중요하다.
- 각 피처의 분포를 알아보기 보다 각 피처의 고윳값별 타깃값 1 비율을 확인하여 해당 피처가 모델링에 필요한지에 대한 여부를 확인한다.
- 고윳값별 타깃값 1 비율이 충분히 차이가 나고 신뢰구간도 작은 피처를 선별한다.

**이진 피처**
<p align="center">
  <img src=./images/binary_feature.png>
</p>

- ps_ind_06_bin
    - 값이 0일 때 타깃값의 비율: 타깃값 0(96%), 타깃값 1(4%)
    - 값이 1일 때 타깃값의 비율: 타깃값 0(97.2%), 타깃값 1(2.8%)
    - 고윳값별로 타깃값의 비율이 다르기 때문에 타깃값을 추정하는 예측력이 있는 것으로 판단된다. 신뢰구간 또한 좁기 때문에 모델링 시 도움이 될 것으로 판단된다.

- ps_ind_10_bin ~ ps_ind_13_bin
    - 신뢰구간이 넓어 통계적 유효성이 떨어지므로 제거하도록 한다.

- ps_calc_15_bin ~ ps_calc_20_bin
    - 고윳값별 타깃값 비율 차이가 없으므로 타깃값 예측력이 없을 것으로 판단되므로 제거하도록 한다.
    - calc 분류의 이진 피처는 모두 타깃값 비율에 차이가 없으므로 이후 다른 피처도 차이가 없는지 확인해본다.

**명목형 피처**
<p align="center">
  <img src=./images/nom_features.png>
</p>

- -1 값을 포함하는 즉, 결측값을 나타내는 피처가 많으므로, 결측값이 많지 않다면 다른 값으로 대체하고, 결측값이 많다면 해당 피처를 제거한다. 하지만, 결측값 자체가 타깃값 예측에 도움을 줄 수도 있다는 점을 생각하자.

- ps_ind_02_cat
    - 결측값 -1이 다른 고윳값들보다 타깃값 1의 비율이 크다. 결측값 자체가 타깃값에 대한 예측력이 있다고 판단되므로, 그대로 모델링에 사용한다. 즉, -1 또한 하나의 고윳값이라고 간주한다.

- ps_car_02_cat
    - -1일 때 타깃값 1의 비율이 0%이므로, 피처 값이 -1이면 타깃값이 0이라고 판단할 수 있다.

- ps_ind_02_cat, ps_car_02_cat, ps_car_01_cat
    - -1을 제외한 다른 고윳값의 경우 타깃값 1의 비율이 비슷하다. -1의 신뢰구간이 넓지만, -1의 신뢰하한과 다른 고윳값들의 신뢰상한 간 차이가 크므로 이들은 모델링에 필요한 피처로 판단된다.

- ps_car_10_cat
    - 고윳값들간의 타깃값 1의 평균 비율이 비슷하다. 하지만, 고윳값 2의 신뢰구간이 유독 넓으므로 제거하기에 애매하다. 이후, 모델링 시 제거할 때와 그렇지 않을 때를 나눠서 성능을 확인해본다.

- 명목형 피처는 모두 모델링에 이용한다.

**순서형 피처**
<p align="center">
  <img src=./images/order_features.png>
</p>

- ps_ind_01
    - 고윳값 4의 신뢰구간이 다른 고윳값들에 비해 타깃값 1의 비율이 높지만, 신뢰구간이 지나치게 넓으므로 통계적 유효성이 떨어진다. 따라서, 해당 피처는 이후 제거하도록 한다.

- ps_calc_04 ~ 14
    - 고윳값별 타깃값 1의 비율이 비슷하거나, 그렇지 않더라도 신뢰구간이 지나치게 넓으므로 해당 피처들은 모두 제거하도록 한다.

**연속형 피처**

**To-do**
- 연속형 피처는 연속된 값이므로 고윳값이 굉장히 많다. 따라서, 값을 몇 개의 구간으로 나눠서 구간별 타깃값 1의 비율을 구한다.
- ```cut()``` 함수 사용
    - ```pd.cut([1.0, 1.5, 2.1, 2.7, 3.5, 4.0], 3)```
    - 3 : 나눌 구간의 개수 입력
- 연속형 피처 간 '상관관계'를 파악한다.
    - 일반적으로 강한 상관관계를 보이는 두 피처가 있으면 예측력도 비슷하므로 둘 중 하나를 제거한다.

<p align="center">
  <img src=./images/continuos_feature.png>
</p>

- ps_calc_01 ~ 03
    - 구간별 타깃값 비율 차이가 없으므로 제거하도록 한다.

<p align="center">
  <img src=./images/continuos_heatmap.png>
</p>

- ps_car_12 & ps_car_14, ps_reg_02 & ps_ref_03
    - 둘 중 하나를 제거해야 할 만큼의 강한 상관관계는 아니지만, 모델링시 확인해본다.

</br>

## 베이스라인 모델
**To-do**
- LightGBM을 사용한다.

### 피처 엔지니어링
**데이터 합치기**
- 전체 피처 중 원하는 피처만 추출하기 위해 all_features 변수를 따로 저장한다.

**명목형 피처 원-핫 인코딩**
- 명목형 데이터는 고윳값별 순서가 따로 없기 때문에 모든 명목형 피처에 원-핫 인코딩을 적용한다.

**필요 없는 피처 제거**
- 탐색적 데이터 분석에서 파악한 제거해야 할 피처를 제거한다.
    - calc 분류에 속하는 20개 피처
    - ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin, ps_ind_14, ps_car_14
- 남겨진 피처를 포함하는 데이터와 앞서 명목형 피처를 원-핫 인코딩한 encoded_cat_matrix를 합친다.


### 평가지표 계산 함수 작성
**To-do**
- 지니계수는 파이썬이나 사이킷런에서 기본으로 제공되지 않으므로 직접 만들어서 사용한다.

> 지니계수란?
> - 경제학 : 소득 불평등 정도를 나타내는 지표
>    - 작을수록 소득 수준이 평등하고, 클수록 불평등하다는 것을 의미
> - 로렌츠 곡선을 이용해 계산
>    - 로렌츠 곡선이란 인구 누적 비율과 해당 소득 누적 점유율을 연결한 선을 의미
> - 머신러닝 : 모델의 예측 성능을 측정하는데 사용
>    - 예측값을 크기순으로 정렬해서 로렌츠 곡선을 구함
>    - 2 x ROC AUC - 1
>    - 평가지표가 지니계수이면 ROC AUC의 상황과 거의 비슷

**정규화 지니계수 계산 함수**
- 정규화 지니계수 = 예측값에 대한 지니계수 / 예측이 완벽할 때의 지니계수
    - 예측값에 대한 지니계수 : 예측값과 실젯값으로 구한 지니계수
    - 예측이 완벽할 때의 지니계수 : 실젯값과 실젯값으로 구한 지니계수


### 모델 훈련 및 성능 검증
**OOF(Out of Fold prediction) 예측 방식**  
K-폴드 교차 검증을 수행하면서 각 폴드마다 테스트 데이터로 예측하는 방식

**예측 절차**
1. 전체 훈련 데이터를 K개 그룹으로 나눈다.
2. K개 그룹 중 한 그룹은 검증 데이터, 나머지 K-1개 그룹은 훈련 데이터로 지정한다.
3. 훈련 데이터로 모델을 훈련한다.
4. 훈련된 모델을 이용해 검증 데이터로 타깃 확률을 예측하고, 전체 테스트 데이터로도 타깃 확률을 예측한다.
5. 검증 데이터로 구한 예측 확률과 테스트 데이터로 구한 예측 확률을 기록한다.
6. 검증 데이터를 다른 그룹으로 바꿔가며 2 ~ 5번 절차를 총 K번 반복한다.
7. K개 그룹의 검증 데이터로 예측한 확률을 훈련 데이터 실제 타깃값과 비교해 성능 평가점수를 계산한다. 이 점수로 모델 성능을 가늠해볼 수 있다.
8. 테스트 데이터로 구한 K개 예측 확률의 평균을 구한다. 이 값이 최종 예측 확률이며, 제출해야 하는 값이다.

**장점**
1. 과대적합 방지 효과가 있다.
2. 앙상블 효과가 있어 모델 성능이 좋아진다.


### OOF방식으로 LightGBM 훈련
**To-do**
- 타깃값이 불균형하기 때문에 층화 K 폴드 교차 검증기를 생성한다. **StratifiedKFold()**

-> OOF 검증 데이터 지니계수:  0.2804995714877777

**제출 결과**
<p align="center">
  <img src=./images/base_result.png>
</p>

</br>

## 성능 개선 1
**To-do**
- 피처 엔지니어링 & 하이퍼파라미터 최적화 적용
    - 파생 피처 추가, 베이지안 최적화

### 피처 엔지니어링
데이터 합치기 -> 명목형 피처 원-핫 인코딩

**파생 피처 추가**
- 한 데이터가 가진 결측값 개수를 파생 피처로 만들어본다.
- ind 분류의 피처를 살펴본다.
    - 18개의 모든 ind 피처를 연결한 새로운 피처를 만든다.
- 명목형 피처의 고윳값별 개수를 새로운 피처로 추가한다.

필요 없는 피처 제거 -> 데이터 나누기

### 하이퍼파라미터 최적화
**To-do**
- 베이지안 최적화 기법을 활용해 하이퍼파라미터를 조정한다.

데이터셋 준비 -> 하이퍼파라미터 범위 설정 -> (베이지안 최적화용) 평가지표 계산 함수 작성 -> 최적화 수행

### 모델 훈련 및 성능 검증
**To-do**
- 그리드서치와 달리 베이지안 최적화는 최적화된 하이퍼파라미터로 훈련된 모델을 제공하지 않으므로 찾은 하이퍼파라미터를 활용해서 모델을 다시 훈련한다.

-> OOF 검증 데이터 지니계수 :  0.2889651000887542

**제출 결과**
<p align="center">
  <img src=./images/improve1_result.png>
</p>

</br>

## 성능 개선 2
**To-do**
- 모델을 XGBoost로 변경한다.
    - 전반적은 프로세스는 앞과 같으나 몇 가지 수정을 한다.
        - 지니계수 반환값 : XGBoost용 지니계수 계산 함수는 반환값이 평가지표명, 평가점수 2개이다. '평가점수가 높으면 좋은지 여부'는 XGBoost 모델 객체의 train() 메서드에 따로 전달해야 한다.
        - 데이터셋 객체
        - 모델 하이퍼파라미터명

### 피처 엔지니어링
LightGBM에서 XGBoost용 gini() 함수로 다시 작성한다.

### 하이퍼파라미터 최적화
데이터셋 준비 -> 하이퍼파라미터 범위 설정 -> (베이지안 최적화용) 평가지표 계산 함수 작성 -> 최적화 수행

### 모델 훈련 및 성능 검증
OOF 검증 데이터 지니계수 :  0.2860206133547507

**제출 결과**
<p align="center">
  <img src=./images/improve2_result.png>
</p>

</br>

## 성능 개선 3
**To-do**
- LightGBM, XGBoost 모델의 예측값을 결합하는 앙상블 기법을 적용해본다.

**제출 결과**
<p align="center">
  <img src=./images/improve3_result.png>
</p>